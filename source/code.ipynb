{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gensim # thư viện NLP\n",
    "import os \n",
    "from nltk.tokenize import word_tokenize\n",
    "doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "data_path = os.path.join(dir_path, 'data\\\\train_set_ielts')\n",
    "test_path = os.path.join(dir_path, 'data\\\\test_set_ielts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Projects\\\\Python\\\\topic-classification\\\\data\\\\test_set_ielts'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path\n",
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_stopword(tokenized_doc,stop_words):\n",
    "    tok_without_sw=[]\n",
    "    for txt_tokens in tokenized_doc:\n",
    "        tok_without_sw = [word for word in tokenized_doc if not word.lower() in STOPWORDS]\n",
    "    return tok_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder_path):\n",
    "    dirs = os.listdir(folder_path)\n",
    "    processed_doc = []\n",
    "    for path in tqdm(dirs):\n",
    "        file_paths = os.listdir(os.path.join(folder_path, path))\n",
    "        for file_path in tqdm(file_paths):\n",
    "            with open(os.path.join(folder_path, path, file_path), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                lines = ' '.join(lines)\n",
    "                lines = gensim.utils.simple_preprocess(lines)\n",
    "                lines = ' '.join(lines)\n",
    "\n",
    "                # Tokenization of each document\n",
    "                tokenized_doc = word_tokenize(lines.lower())\n",
    "                #remove stop word\n",
    "                tokenized_doc = rm_stopword(tokenized_doc, STOPWORDS)\n",
    "                processed_doc.append([tokenized_doc,path])\n",
    "    return processed_doc\n",
    "\n",
    "def run_test(model, test_doc):\n",
    "    test_pass = 0\n",
    "    for index in range(len(test_doc)):\n",
    "        result = model.docvecs.most_similar(positive=[model.infer_vector(test_doc[index][0])],topn=6)\n",
    "        if (result[0][0] == test_doc[index][1]):\n",
    "            test_pass+= 1\n",
    "    return [test_pass,len(test_doc)+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 286.55it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 298.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 514.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 640.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 500.78it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 510.97it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 35.02it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 192.43it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 192.05it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 166.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 333.97it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 447.04it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 598.30it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 71.57it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = get_data(data_path)\n",
    "data_test = get_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_doc =[]\n",
    "for case in range(len(data_train)):\n",
    "    case_i = TaggedDocument(data_train[case][0],[data_train[case][1]])\n",
    "    tagged_doc.append(case_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train doc2vec model\n",
    "model = Doc2Vec(tagged_doc, vector_size=300, window=2, min_count=1, workers=cores, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained doc2vec model\n",
    "model_path = os.path.dirname(os.path.realpath(os.getcwd()))\n",
    "model_path = os.path.join(model_path, 'models\\\\model_doc2vec_ielts.model')\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved doc2vec model\n",
    "model= Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 17]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "pass_percent = run_test(model, data_test)\n",
    "pass_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most similar doc topic\n",
    "\n",
    "text_check = \"\"\"\n",
    "Some believe children should be taught to give speeches and presentations in school.\n",
    "\n",
    "Why is this?\n",
    "\n",
    "Should this be taught in schools?\n",
    "\n",
    "Many believe that giving presentations and speeches ought to be a key part of school curricula for children. In my opinion, this is an attempt to develop a number of skills holistically and should be encouraged.\n",
    "\n",
    "The main reason schools incorporate presentations is to improve skills needed for the future. Children will later be expected to present individually and in groups throughout their academic career and in most work contexts. In order to give a competent presentation, children must first of all develop confidence speaking in front of others and then combine this with careful preparation, repeated practice, research, and, often, team-working skills. Each of these qualities will be useful later and the earlier students begin, the more likely they are to excel in areas that many adults still find challenging.\n",
    "\n",
    "I would recommend this practice continues since integration of skills contributes to greater progress. Skills developed on their own are often not as memorable. If a young child, for example, must do a book report with a group of other children this requires them to read the book, divide up sections of the presentation, communicate with team members, and deliver an engaging speech at the end. The combination of all skills makes the learning more memorable and likely to develop fixed characteristics. An illustrative analogy would be how an athlete practices for a sport. They can master individual skills on their own but the greatest progress comes when they blend them under the intense pressure of a game.\n",
    "\n",
    "In conclusion, educators often teach public speaking in order to prepare students for the future and this multidisciplinary approach is a positive. Presentations and speeches are also a good way to combine and review past lessons.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('education', 0.7760127782821655),\n",
       " ('crime and punishment', 0.6115237474441528),\n",
       " ('bussiness', 0.5060230493545532),\n",
       " ('health', 0.48205405473709106),\n",
       " ('enviroment', 0.45736056566238403),\n",
       " ('technology', 0.3190247118473053)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Doc2Vec.load(model_path)\n",
    "test_doc = word_tokenize(text_check.lower())\n",
    "result = model.docvecs.most_similar(positive=[model.infer_vector(test_doc)],topn=6)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2a992d86943bdbb1a93721aa8faf11a2d48a4b1fd05bac4b69f4b63e0ef12fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
